{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instances where we use:     y = x · A^T + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene expression analysis: Transforming gene expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Simulated gene expression data\n",
    "num_samples = 100\n",
    "num_genes = 1000\n",
    "gene_expression = torch.randn(num_samples, num_genes)\n",
    "\n",
    "# Linear transformation for gene expression analysis\n",
    "output_dim = 50\n",
    "A = torch.randn(output_dim, num_genes)\n",
    "b = torch.randn(output_dim)\n",
    "\n",
    "# Transform gene expression data\n",
    "transformed_expression = torch.matmul(gene_expression, A.t()) + b\n",
    "\n",
    "print(\"Original shape:\", gene_expression.shape)\n",
    "print(\"Transformed shape:\", transformed_expression.shape)\n",
    "\n",
    "# Example analysis: Find genes with highest average expression\n",
    "avg_expression = torch.mean(transformed_expression, dim=0)\n",
    "top_genes = torch.argsort(avg_expression, descending=True)[:10]\n",
    "print(\"Top 10 gene indices after transformation:\", top_genes.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Structure Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Simulated protein features (e.g., amino acid properties)\n",
    "num_residues = 200\n",
    "feature_dim = 20\n",
    "protein_features = torch.randn(1, num_residues, feature_dim)\n",
    "\n",
    "# Linear transformation for structure prediction\n",
    "output_dim = 3  # 3D coordinates\n",
    "A = torch.randn(output_dim, feature_dim)\n",
    "b = torch.randn(output_dim)\n",
    "\n",
    "# Predict 3D coordinates\n",
    "predicted_structure = torch.matmul(protein_features, A.t()) + b\n",
    "\n",
    "print(\"Protein features shape:\", protein_features.shape)\n",
    "print(\"Predicted structure shape:\", predicted_structure.shape)\n",
    "\n",
    "# Visualize first few predicted coordinates\n",
    "print(\"First 5 predicted 3D coordinates:\")\n",
    "print(predicted_structure[0, :5, :].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Analysis (DNA encoding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# DNA sequence\n",
    "dna_seq = \"ATCGATCGATCG\"\n",
    "\n",
    "# One-hot encoding\n",
    "nucleotide_to_index = {'A': 0, 'T': 1, 'C': 2, 'G': 3}\n",
    "one_hot = torch.zeros(len(dna_seq), 4)\n",
    "for i, nucleotide in enumerate(dna_seq):\n",
    "    one_hot[i, nucleotide_to_index[nucleotide]] = 1\n",
    "\n",
    "# Linear transformation for sequence analysis\n",
    "output_dim = 8\n",
    "A = torch.randn(output_dim, 4)\n",
    "b = torch.randn(output_dim)\n",
    "\n",
    "# Transform encoded sequence\n",
    "transformed_seq = torch.matmul(one_hot, A.t()) + b\n",
    "\n",
    "print(\"One-hot encoded shape:\", one_hot.shape)\n",
    "print(\"Transformed sequence shape:\", transformed_seq.shape)\n",
    "\n",
    "# Example analysis: Find position with highest transformed value\n",
    "max_pos = torch.argmax(torch.max(transformed_seq, dim=1)[0])\n",
    "print(\"Position with highest transformed value:\", max_pos.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of what happens during dimensionality reduction:\n",
    "\n",
    "Original Data:\n",
    "\n",
    "We start with high-dimensional data (100 dimensions in this case).\n",
    "Each data point is represented by 100 features, which is difficult to visualize directly.\n",
    "The first plot shows only the first 3 dimensions of this 100-dimensional space.\n",
    "\n",
    "\n",
    "Dimensionality Reduction:\n",
    "\n",
    "We reduce the 100-dimensional data to 2 dimensions.\n",
    "This process attempts to preserve the most important information or patterns in the data.\n",
    "\n",
    "\n",
    "Our Linear Transformation Method:\n",
    "\n",
    "We use the equation y = x · A^T + b to transform the data.\n",
    "This method projects the high-dimensional data onto a 2D plane.\n",
    "The resulting plot shows how the data points are distributed in this new 2D space.\n",
    "However, this random linear transformation may not optimally preserve the data structure.\n",
    "\n",
    "\n",
    "PCA (Principal Component Analysis) Method:\n",
    "\n",
    "PCA finds the directions (principal components) of maximum variance in the data.\n",
    "It then projects the data onto these principal components.\n",
    "The resulting plot shows the data distributed along the two most significant principal components.\n",
    "PCA is often more effective at preserving the overall structure of the data.\n",
    "\n",
    "We added plt.ion() at the beginning to enable interactive mode. We removed the plt.savefig() calls and kept the plt.show() calls. We added plt.ioff() and a final plt.show() at the end to keep the plot windows open.\n",
    "\n",
    "When you run this script in PyCharm:\n",
    "\n",
    "The plots should appear in the \"SciView\" tool window (usually on the right side of the PyCharm window).\n",
    "If the plots don't appear automatically, you might need to click on the \"Python Scientific\" tab in the tool window.\n",
    "You can interact with the plots, zoom in/out, and pan around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Enable interactive mode for matplotlib\n",
    "plt.ion()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulated high-dimensional biological data\n",
    "num_samples = 1000\n",
    "original_dim = 100\n",
    "data = torch.randn(num_samples, original_dim)\n",
    "\n",
    "# Linear transformation for dimensionality reduction\n",
    "reduced_dim = 2\n",
    "A = torch.randn(reduced_dim, original_dim)\n",
    "b = torch.randn(reduced_dim)\n",
    "\n",
    "# Reduce dimensionality using our linear transformation\n",
    "reduced_data = torch.matmul(data, A.t()) + b\n",
    "\n",
    "print(\"Original data shape:\", data.shape)\n",
    "print(\"Reduced data shape:\", reduced_data.shape)\n",
    "\n",
    "# Use PCA for comparison\n",
    "pca = PCA(n_components=2)\n",
    "pca_reduced_data = pca.fit_transform(data.numpy())\n",
    "\n",
    "# Plot 1: Original vs Our Method\n",
    "fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Original data (first 2 dimensions)\n",
    "ax1.scatter(data[:, 0].numpy(), data[:, 1].numpy(), alpha=0.5)\n",
    "ax1.set_title(\"Original Data (First 2 Dimensions)\")\n",
    "ax1.set_xlabel(\"Dimension 1\")\n",
    "ax1.set_ylabel(\"Dimension 2\")\n",
    "\n",
    "# Reduced data (our method)\n",
    "ax2.scatter(reduced_data[:, 0].numpy(), reduced_data[:, 1].numpy(), alpha=0.5)\n",
    "ax2.set_title(\"Reduced Data (Our Method)\")\n",
    "ax2.set_xlabel(\"Dimension 1\")\n",
    "ax2.set_ylabel(\"Dimension 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Original vs PCA\n",
    "fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Original data (first 2 dimensions)\n",
    "ax1.scatter(data[:, 0].numpy(), data[:, 1].numpy(), alpha=0.5)\n",
    "ax1.set_title(\"Original Data (First 2 Dimensions)\")\n",
    "ax1.set_xlabel(\"Dimension 1\")\n",
    "ax1.set_ylabel(\"Dimension 2\")\n",
    "\n",
    "# Reduced data (PCA)\n",
    "ax2.scatter(pca_reduced_data[:, 0], pca_reduced_data[:, 1], alpha=0.5)\n",
    "ax2.set_title(\"Reduced Data (PCA)\")\n",
    "ax2.set_xlabel(\"Principal Component 1\")\n",
    "ax2.set_ylabel(\"Principal Component 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Explained variance ratio (PCA):\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Keep the plot windows open\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
